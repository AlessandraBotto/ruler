{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ruler_user_study_figures.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOcNNjdHH6+nmJche/DTXa7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rulerauthors/ruler/blob/master/user_study/ruler_user_study_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrzzXDLCIg8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EigrKeTbJ3Xo",
        "colab_type": "text"
      },
      "source": [
        "## Load the full user study data from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVXo3EbXJlWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/full_study_data.csv')\n",
        "display(full_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TactFI4vNNa8",
        "colab_type": "text"
      },
      "source": [
        "## About this data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5uuYGwDNlUr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> We carried out the study using  a within-subjects experiment design, where all participants performed tasks using both conditions (tools).  The sole independent variable controlled was the method of creating labeling functions. We counterbalanced the order in which the tools were used, as well as which classification task we performed with which tool. \n",
        "\n",
        "### Tasks and Procedure \n",
        "> We asked participants to write  labeling functions for two prevalent labeling tasks: spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. Participants received 15 mins of instruction on how to use each tool, using a topic classification task (electronics vs. guns) over a newsgroup dataset~\\cite{rennie200820} as an example. We asked participants to write as many functions as they considered necessary for the goal of the task.  There were given 30 mins to complete each task and we recorded the labeling functions they created and these functions' individual and aggregate performances.  After completing both tasks, participants also filled out an exit survey, providing their qualitative feedback.\n",
        "\n",
        "> For the manual programming condition, we iteratively developed a Jupyter notebook interface based on the Snorkel tutorial. We provided a section for writing functions, a section with diverse analysis tools, and a section to train a logistic regression model on the labels they had generated (evaluated on the test set shown to the user, which is separate from our heldout test set used for the final evaluation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6VOJnAuJ9fw",
        "colab_type": "text"
      },
      "source": [
        "## Select Best Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqInQrTuMlUn",
        "colab_type": "text"
      },
      "source": [
        "From [our EMNLP '20 submission](https://github.com/rulerauthors/ruler/blob/master/media/Ruler_EMNLP2020.pdf):\n",
        "\n",
        "\n",
        "\n",
        "> To analyze the performance of the labeling functions created by participants, for each participant we select and task the labeling  model  that achieved the highest f1 score on the development set.  For each labeling model, we then train a logistic regression model on a training dataset  generated by the model.  We finally evaluate the performance of the logistic regression model on a heldout test set. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXYvcdQlJn8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_best_table_small(action='heldout_test_LR_stats'):\n",
        "  dt = pd.DataFrame()\n",
        "\n",
        "  subjects = full_data.participant.value_counts().index\n",
        "  datasets = ['amazon', 'youtube']\n",
        "\n",
        "  for _, pid in enumerate(subjects):\n",
        "    for d in datasets: \n",
        "      # gather all the rows logging participant {pid}'s progress on the given dataset/task\n",
        "      sub_df = full_data[(full_data['participant']==pid) & (full_data['dataset']==d)]\n",
        "      sub_df = sub_df.reset_index(drop=True)\n",
        "\n",
        "      # find index of best performance on dev set\n",
        "      idxmax = sub_df[sub_df.data == 'dev']['f1'].idxmax()\n",
        "\n",
        "      # choose the first logistic regression model trained after that,\n",
        "      # report the performance on the held out test data\n",
        "      try:\n",
        "        r = sub_df.loc[idxmax:][sub_df.action==action].iloc[0]\n",
        "      except IndexError:\n",
        "        # in one case the user never finished any labelling functions, \n",
        "        # so we report the initial 'baseline' LR performance\n",
        "        # which is f1 score of 0.5\n",
        "        r = sub_df[sub_df.action==action].iloc[0]\n",
        "\n",
        "      # the logged precision and recall are separated by class. \n",
        "      # we use the heldout dataset splits to compute micro precision and recall\n",
        "      size0 = 418\n",
        "      size1 = 382\n",
        "      if r.task==\"Youtube\":\n",
        "        size0=192\n",
        "        size1=164\n",
        "      prec = (r['precision_0']*size0 +r['precision_1']*size1)/(size0+size1)\n",
        "      rec = (r['recall_0']*size0+r['recall_1']*size1)/(size0+size1)\n",
        "\n",
        "      dt = dt.append({'participant': pid, \n",
        "                      'condition': r['condition'].lower(),\n",
        "                      'task':'sentiment' if d == 'amazon' else 'spam', \n",
        "                      'dataset':d,\n",
        "                      'f1':r['micro_f1'],\n",
        "                      'precision':prec, \n",
        "                      'recall':rec, \n",
        "                      'accuracy':r['accuracy'],\n",
        "                      'max_dev_f1': sub_df.at[idxmax, 'f1'],\n",
        "                      'training_label_coverage': r['training_label_coverage'],\n",
        "                      }, ignore_index=True)\n",
        "  return dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_2nl8UJ1KB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "afb2fe06-37c6-491c-8c33-f7d71e2de999"
      },
      "source": [
        "dt_best_small  = create_best_table_small()\n",
        "display(dt_best_small)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>condition</th>\n",
              "      <th>dataset</th>\n",
              "      <th>f1</th>\n",
              "      <th>max_dev_f1</th>\n",
              "      <th>participant</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>task</th>\n",
              "      <th>training_label_coverage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.483750</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.562036</td>\n",
              "      <td>0.653386</td>\n",
              "      <td>p4</td>\n",
              "      <td>0.492273</td>\n",
              "      <td>0.483750</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.58375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.578652</td>\n",
              "      <td>ruler</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.713740</td>\n",
              "      <td>0.632558</td>\n",
              "      <td>p4</td>\n",
              "      <td>0.682599</td>\n",
              "      <td>0.525599</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.13500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.482500</td>\n",
              "      <td>ruler</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.622951</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>p8</td>\n",
              "      <td>0.501770</td>\n",
              "      <td>0.482500</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.11250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.789326</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.764890</td>\n",
              "      <td>0.697436</td>\n",
              "      <td>p8</td>\n",
              "      <td>0.821485</td>\n",
              "      <td>0.809982</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.54375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.506250</td>\n",
              "      <td>ruler</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.655623</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>p2</td>\n",
              "      <td>0.667621</td>\n",
              "      <td>0.506250</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.40250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.679775</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.604167</td>\n",
              "      <td>0.691892</td>\n",
              "      <td>p2</td>\n",
              "      <td>0.744225</td>\n",
              "      <td>0.710193</td>\n",
              "      <td>spam</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.588750</td>\n",
              "      <td>ruler</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.636464</td>\n",
              "      <td>0.644860</td>\n",
              "      <td>p1</td>\n",
              "      <td>0.608134</td>\n",
              "      <td>0.588750</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.29750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.528090</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.596154</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>p1</td>\n",
              "      <td>0.517664</td>\n",
              "      <td>0.512288</td>\n",
              "      <td>spam</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.510000</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.565410</td>\n",
              "      <td>0.639456</td>\n",
              "      <td>p3</td>\n",
              "      <td>0.519668</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.612360</td>\n",
              "      <td>ruler</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>p3</td>\n",
              "      <td>0.695697</td>\n",
              "      <td>0.566626</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.62375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.565657</td>\n",
              "      <td>p7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.710674</td>\n",
              "      <td>ruler</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.736573</td>\n",
              "      <td>0.751220</td>\n",
              "      <td>p7</td>\n",
              "      <td>0.708282</td>\n",
              "      <td>0.705396</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.57875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.491250</td>\n",
              "      <td>ruler</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.647619</td>\n",
              "      <td>0.614786</td>\n",
              "      <td>p0</td>\n",
              "      <td>0.598714</td>\n",
              "      <td>0.491250</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.63250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.539326</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.699634</td>\n",
              "      <td>0.687225</td>\n",
              "      <td>p0</td>\n",
              "      <td>0.518884</td>\n",
              "      <td>0.478199</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.27375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.515000</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.649186</td>\n",
              "      <td>0.648276</td>\n",
              "      <td>p9</td>\n",
              "      <td>0.601146</td>\n",
              "      <td>0.515000</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.38875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.679775</td>\n",
              "      <td>ruler</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.707692</td>\n",
              "      <td>0.769874</td>\n",
              "      <td>p9</td>\n",
              "      <td>0.676727</td>\n",
              "      <td>0.674545</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.57750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.630000</td>\n",
              "      <td>ruler</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.543210</td>\n",
              "      <td>0.672727</td>\n",
              "      <td>p6</td>\n",
              "      <td>0.636876</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.912921</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.915531</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>p6</td>\n",
              "      <td>0.911618</td>\n",
              "      <td>0.918011</td>\n",
              "      <td>spam</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.658750</td>\n",
              "      <td>snorkel</td>\n",
              "      <td>amazon</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.705394</td>\n",
              "      <td>p5</td>\n",
              "      <td>0.683448</td>\n",
              "      <td>0.658750</td>\n",
              "      <td>sentiment</td>\n",
              "      <td>0.96500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.533708</td>\n",
              "      <td>ruler</td>\n",
              "      <td>youtube</td>\n",
              "      <td>0.695971</td>\n",
              "      <td>0.701299</td>\n",
              "      <td>p5</td>\n",
              "      <td>0.256285</td>\n",
              "      <td>0.472526</td>\n",
              "      <td>spam</td>\n",
              "      <td>0.53000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy condition  dataset        f1  max_dev_f1 participant  precision  \\\n",
              "0   0.483750   snorkel   amazon  0.562036    0.653386          p4   0.492273   \n",
              "1   0.578652     ruler  youtube  0.713740    0.632558          p4   0.682599   \n",
              "2   0.482500     ruler   amazon  0.622951    0.588235          p8   0.501770   \n",
              "3   0.789326   snorkel  youtube  0.764890    0.697436          p8   0.821485   \n",
              "4   0.506250     ruler   amazon  0.655623    0.642857          p2   0.667621   \n",
              "5   0.679775   snorkel  youtube  0.604167    0.691892          p2   0.744225   \n",
              "6   0.588750     ruler   amazon  0.636464    0.644860          p1   0.608134   \n",
              "7   0.528090   snorkel  youtube  0.596154    0.651163          p1   0.517664   \n",
              "8   0.510000   snorkel   amazon  0.565410    0.639456          p3   0.519668   \n",
              "9   0.612360     ruler  youtube  0.726190    0.750000          p3   0.695697   \n",
              "10  0.500000   snorkel   amazon  0.500000    0.565657          p7   0.000000   \n",
              "11  0.710674     ruler  youtube  0.736573    0.751220          p7   0.708282   \n",
              "12  0.491250     ruler   amazon  0.647619    0.614786          p0   0.598714   \n",
              "13  0.539326   snorkel  youtube  0.699634    0.687225          p0   0.518884   \n",
              "14  0.515000   snorkel   amazon  0.649186    0.648276          p9   0.601146   \n",
              "15  0.679775     ruler  youtube  0.707692    0.769874          p9   0.676727   \n",
              "16  0.630000     ruler   amazon  0.543210    0.672727          p6   0.636876   \n",
              "17  0.912921   snorkel  youtube  0.915531    0.947368          p6   0.911618   \n",
              "18  0.658750   snorkel   amazon  0.695652    0.705394          p5   0.683448   \n",
              "19  0.533708     ruler  youtube  0.695971    0.701299          p5   0.256285   \n",
              "\n",
              "      recall       task  training_label_coverage  \n",
              "0   0.483750  sentiment                  0.58375  \n",
              "1   0.525599       spam                  0.13500  \n",
              "2   0.482500  sentiment                  0.11250  \n",
              "3   0.809982       spam                  0.54375  \n",
              "4   0.506250  sentiment                  0.40250  \n",
              "5   0.710193       spam                  1.00000  \n",
              "6   0.588750  sentiment                  0.29750  \n",
              "7   0.512288       spam                  1.00000  \n",
              "8   0.510000  sentiment                  1.00000  \n",
              "9   0.566626       spam                  0.62375  \n",
              "10  0.000000  sentiment                  0.00000  \n",
              "11  0.705396       spam                  0.57875  \n",
              "12  0.491250  sentiment                  0.63250  \n",
              "13  0.478199       spam                  0.27375  \n",
              "14  0.515000  sentiment                  0.38875  \n",
              "15  0.674545       spam                  0.57750  \n",
              "16  0.630000  sentiment                  1.00000  \n",
              "17  0.918011       spam                  1.00000  \n",
              "18  0.658750  sentiment                  0.96500  \n",
              "19  0.472526       spam                  0.53000  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sHa8J1lLuIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWaGsxu-NzXM",
        "colab_type": "text"
      },
      "source": [
        "## Generate Figures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wWpsd_bPcE",
        "colab_type": "text"
      },
      "source": [
        "### Quantitative (model performance, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuRwkwc2N2XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_bm_small = dt_best_small.melt(id_vars=['participant', 'condition', 'task', 'dataset'], \n",
        "        var_name=\"metric\", \n",
        "        value_name=\"value\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP01XKYMN3CF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "e655cfad-ffc7-4437-c3fa-ee9adef88a70"
      },
      "source": [
        "aW = 300\n",
        "H = 50\n",
        "error_bars = alt.Chart(dt_bm_small).mark_errorbar(extent='stderr').encode(\n",
        "  x=alt.X('value:Q'),\n",
        "  y=alt.Y('condition:N'),\n",
        "  color=alt.Color('condition:N', sort=['ruler'])\n",
        ").properties(width=W,height=H)\n",
        "\n",
        "points = alt.Chart(dt_bm_small).mark_point(filled=True).encode(\n",
        "  x=alt.X('value:Q', title=None, aggregate='mean', axis=alt.Axis(tickCount=10)),\n",
        "  y=alt.Y('condition:N'),\n",
        "  text=alt.Text('value:Q'),\n",
        "  color=alt.Color('condition:N', sort=['ruler'], legend=alt.Legend(title=None, orient='top'))\n",
        ").properties(width=W,height=H)\n",
        "\n",
        "(error_bars + points).facet(\n",
        "    facet= alt.Facet('metric:N', sort=['f1', 'accuracy', 'training_label_coverage', 'max_dev_f1'], title=None),\n",
        "    columns=2\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.FacetChart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-2bbc7b85bf794b3da55e7d30a0773e03\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-2bbc7b85bf794b3da55e7d30a0773e03\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-2bbc7b85bf794b3da55e7d30a0773e03\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-277cd1406d98bcba4045d756263dfc50\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"metric\", \"sort\": [\"f1\", \"accuracy\", \"training_label_coverage\", \"max_dev_f1\"], \"title\": null}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 50, \"width\": 300}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"legend\": {\"orient\": \"top\", \"title\": null}, \"sort\": [\"ruler\"]}, \"text\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"axis\": {\"tickCount\": 10}, \"field\": \"value\", \"title\": null}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 50, \"width\": 300}]}, \"columns\": 2, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-277cd1406d98bcba4045d756263dfc50\": [{\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.48375}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5786516853932584}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.4825}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.7893258426966292}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.50625}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.6797752808988764}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.58875}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5280898876404494}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.51}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.6123595505617978}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.5}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.7106741573033708}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.49125}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5393258426966292}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.515}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.6797752808988764}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.63}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.9129213483146068}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"accuracy\", \"value\": 0.65875}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"accuracy\", \"value\": 0.5337078651685393}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5620360551431601}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7137404580152672}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6229508196721313}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7648902821316614}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6556233653007847}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.6041666666666666}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6364640883977901}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.5961538461538463}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5654101995565409}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7261904761904762}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7365728900255755}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6476190476190475}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.6996336996336996}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6491862567811935}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.7076923076923077}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.5432098765432098}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.9155313351498636}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"f1\", \"value\": 0.6956521739130435}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"f1\", \"value\": 0.6959706959706959}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.653386454183267}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6325581395348836}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.5882352941176471}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6974358974358974}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6428571428571429}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6918918918918918}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6448598130841121}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6511627906976744}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6394557823129251}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7500000000000001}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.5656565657}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7512195121951221}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6147859922178989}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.6872246696035242}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6482758620689655}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7698744769874477}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.6727272727272727}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.9473684210526316}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"max_dev_f1\", \"value\": 0.7053941908713692}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"max_dev_f1\", \"value\": 0.7012987012987013}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.4922725221697655}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.6825991465863452}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.5017700851290237}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.8214846301963346}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6676213818860878}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.7442247596153845}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6081340813551366}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.5176636904761905}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.5196675824175824}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.6956971153846154}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.0}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.7082824152610185}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.5987136457285228}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.5188841807909604}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.601146408839779}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.6767270809359417}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6368761792120751}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.9116182320441989}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"precision\", \"value\": 0.6834482200647249}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"precision\", \"value\": 0.2562853107344633}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.48375}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.5255986407520326}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.4825}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.8099815802845528}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.5062500000000001}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.710193407012195}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.58875}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.512287855691057}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.51}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.5666263338414634}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.0}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.7053963414634146}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.49125}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.4781989964430894}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.515}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.674544588414634}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.6299999999999999}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.9180106707317072}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"recall\", \"value\": 0.65875}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"recall\", \"value\": 0.47252604166666673}, {\"participant\": \"p4\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.58375}, {\"participant\": \"p4\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.135}, {\"participant\": \"p8\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.1125}, {\"participant\": \"p8\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.54375}, {\"participant\": \"p2\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.4025}, {\"participant\": \"p2\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p1\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.2975}, {\"participant\": \"p1\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p3\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p3\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.62375}, {\"participant\": \"p7\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.0}, {\"participant\": \"p7\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.57875}, {\"participant\": \"p0\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.6325}, {\"participant\": \"p0\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.27375}, {\"participant\": \"p9\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.38875}, {\"participant\": \"p9\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.5775}, {\"participant\": \"p6\", \"condition\": \"ruler\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p6\", \"condition\": \"snorkel\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 1.0}, {\"participant\": \"p5\", \"condition\": \"snorkel\", \"task\": \"sentiment\", \"dataset\": \"amazon\", \"metric\": \"training_label_coverage\", \"value\": 0.965}, {\"participant\": \"p5\", \"condition\": \"ruler\", \"task\": \"spam\", \"dataset\": \"youtube\", \"metric\": \"training_label_coverage\", \"value\": 0.53}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAnDe8bQbKgk",
        "colab_type": "text"
      },
      "source": [
        "### Qualitative (Survey responses)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmaH-5fSN7XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "background = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/background_survey_anon.csv', index_col=0)\n",
        "exit_survey = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/exit_survey_anon.csv', index_col=0)\n",
        "final_survey = pd.read_csv('https://raw.githubusercontent.com/rulerauthors/ruler/master/user_study/final_survey_anon.csv', index_col=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRrF7zE-dgjo",
        "colab_type": "text"
      },
      "source": [
        "The original column names for exit_survey shows the statements that the users ranked their agreement with, on a Likert scale of 1-5.\n",
        "\n",
        "We'll shorten these column names for our figures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxi4TOm6c16Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simplify column names\n",
        "exit_survey.columns = ['Timestamp', 'condition',\n",
        "       'satisfaction', 'easy to use',\n",
        "       'expressive enough',\n",
        "       'easy to learn',\n",
        "       'feedback',\n",
        "       'how to improve',\n",
        "       'other',\n",
        "       'comments', 'participant']\n",
        "exit_survey = exit_survey.drop('Timestamp', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LykHpTaLcw3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_qm = exit_survey.melt(id_vars=['participant', 'condition','comments', 'how to improve', 'feedback', 'other'], \n",
        "        var_name=\"metric\", \n",
        "        value_name=\"value\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI5rSlHzcGPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "8884e8e3-cab4-459d-ee22-89a0cfbde8e2"
      },
      "source": [
        "error_bars = alt.Chart(df_qm).mark_errorbar(extent='stderr').encode(\n",
        "  x=alt.X('value:Q'),\n",
        "  y=alt.Y('condition:N'),\n",
        "      color=alt.Color('condition:N', sort=['ruler'])\n",
        ").properties(width=400,height=100)\n",
        "\n",
        "points = alt.Chart(df_qm).mark_point(filled=True).encode(\n",
        "  x=alt.X('value:Q', aggregate='mean'),\n",
        "  y=alt.Y('condition:N'),\n",
        "    color=alt.Color('condition:N', sort=['ruler'])\n",
        ").properties(width=400,height=100)\n",
        "\n",
        "(error_bars + points).facet(\n",
        "    facet= alt.Facet('metric:N',sort=['ease of use', 'expressivity', 'ease of learning', 'overall']),\n",
        "    columns=2\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.FacetChart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-cc836af0bf0a42f3979fa368372e9b86\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-cc836af0bf0a42f3979fa368372e9b86\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-cc836af0bf0a42f3979fa368372e9b86\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-92a60431689dc2c52e6696b08e6eb8a0\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"metric\", \"sort\": [\"ease of use\", \"expressivity\", \"ease of learning\", \"overall\"]}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"stderr\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 100, \"width\": 400}, {\"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"condition\", \"sort\": [\"ruler\"]}, \"x\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"value\"}, \"y\": {\"type\": \"nominal\", \"field\": \"condition\"}}, \"height\": 100, \"width\": 400}]}, \"columns\": 2, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-92a60431689dc2c52e6696b08e6eb8a0\": [{\"participant\": \"p7\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": null, \"other\": null, \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"satisfaction\", \"value\": 3}, {\"participant\": \"p3\", \"condition\": \"Ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"satisfaction\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"Snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p9\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"satisfaction\", \"value\": 3}, {\"participant\": \"p9\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": null, \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"satisfaction\", \"value\": 3}, {\"participant\": \"p4\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": null, \"metric\": \"satisfaction\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"satisfaction\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p5\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"satisfaction\", \"value\": 3}, {\"participant\": \"p5\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"satisfaction\", \"value\": 5}, {\"participant\": \"p0\", \"condition\": \"Ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": null, \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p0\", \"condition\": \"Snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": null, \"metric\": \"satisfaction\", \"value\": 5}, {\"participant\": \"p1\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"satisfaction\", \"value\": 2}, {\"participant\": \"p1\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": null, \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"satisfaction\", \"value\": 2}, {\"participant\": \"p2\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p8\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": null, \"metric\": \"satisfaction\", \"value\": 3}, {\"participant\": \"p8\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"satisfaction\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": null, \"other\": null, \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to use\", \"value\": 2}, {\"participant\": \"p3\", \"condition\": \"Ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"easy to use\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"Snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p9\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"easy to use\", \"value\": 2}, {\"participant\": \"p9\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": null, \"metric\": \"easy to use\", \"value\": 5}, {\"participant\": \"p4\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"easy to use\", \"value\": 3}, {\"participant\": \"p4\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": null, \"metric\": \"easy to use\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p6\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p5\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to use\", \"value\": 5}, {\"participant\": \"p5\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p0\", \"condition\": \"Ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": null, \"metric\": \"easy to use\", \"value\": 3}, {\"participant\": \"p0\", \"condition\": \"Snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": null, \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p1\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to use\", \"value\": 3}, {\"participant\": \"p1\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": null, \"metric\": \"easy to use\", \"value\": 5}, {\"participant\": \"p2\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"easy to use\", \"value\": 2}, {\"participant\": \"p2\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"easy to use\", \"value\": 5}, {\"participant\": \"p8\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": null, \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p8\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"easy to use\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": null, \"other\": null, \"metric\": \"expressive enough\", \"value\": 2}, {\"participant\": \"p7\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"expressive enough\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"Ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"expressive enough\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"Snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"expressive enough\", \"value\": 5}, {\"participant\": \"p9\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"expressive enough\", \"value\": 5}, {\"participant\": \"p9\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": null, \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": null, \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p6\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p6\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p5\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"expressive enough\", \"value\": 3}, {\"participant\": \"p5\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"expressive enough\", \"value\": 5}, {\"participant\": \"p0\", \"condition\": \"Ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": null, \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p0\", \"condition\": \"Snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": null, \"metric\": \"expressive enough\", \"value\": 5}, {\"participant\": \"p1\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p1\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": null, \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"expressive enough\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"expressive enough\", \"value\": 3}, {\"participant\": \"p8\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": null, \"metric\": \"expressive enough\", \"value\": 2}, {\"participant\": \"p8\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"expressive enough\", \"value\": 2}, {\"participant\": \"p7\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. summary panel for labeling function helping to group and delete/add LFs. 2. adding LFs not by examples should be combined\", \"feedback\": null, \"other\": null, \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p7\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p3\", \"condition\": \"Ruler\", \"comments\": \"Thanks!\", \"how to improve\": \"Below are the bugs we discussed. I am not suggesting they all need to be fixed :).\\n1. When I submitted a value modification for a concept row, the span annotations in the text did not update unless a new span could be identified in the text after the modification. Example: changing \\\"firearm\\\" to \\\"fire\\\" would cause the annotations to update, but a change from \\\"firearm\\\" to \\\"filkjwerlkjsdf\\\" would not cause an update.\\n2. I could not delete concept rows.\\n\\nBelow are some possible improvements I can think of:\\n1. I wasn't always sure when a concept modification had taken effect. It would be nice if there were some indication in the UI that the concept modification was in fact affect the output.\\n2. We could show a plot of the historical performance of the model over time. The plot could be a multi-line chart of statistics in the top-right corner on the y-axis and the chronological change id on the x-axis. Then, if a user were to click on a point in the plot, they would see a modal that would ask them if they would like to download the model from that point in time.\\n3. It would be nice to see class-specific statistics for snorkel's labelling model.\\n4. Should we give the user the option to select \\\"weighted average\\\" or \\\"simple average\\\" for the statistics on the entire development set?\\n5. I do think that organizing labelling functions into concepts helped quite a bit. Users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"feedback\": \"I wonder if I should have added more functions more quickly and done more pruning given the diffs.\\n\\nI do think that organizing labelling functions into concepts helped quite a bit. However, users who are using concepts to organize their functions (and thus modifying their concepts over time) would want to see diffs for the statistics after each concept modification.\", \"other\": \"I thought it was conducted very well!\", \"metric\": \"easy to learn\", \"value\": 5}, {\"participant\": \"p3\", \"condition\": \"Snorkel\", \"comments\": \"We were having trouble determining why Snorkel was telling us we had classified 94 positive results correctly and 0 incorrectly but achieved only 47% accuracy. I think Snorkel was saying we correctly labeled 94 of the 94 actual positive examples, and 47% of the examples we identified as positive were actually positive. In other words, Snorkel was telling us that our recall was 100% and our precision was 47%.\", \"how to improve\": \"They may already have this, but I would add the ability to pass in your own metric definitions to the evaluation step\", \"feedback\": \"What worked:\\n1. Writing my own functions to analyze why labeling functions were making incorrect predictions\\n2. I didn't have time to do this, but I would have probably analyzed the model performance metrics myself. We ran into a problem where we weren't entirely sure how the metrics were being calculated, and so I would probably calculate them myself to have complete understanding.\", \"other\": \"The study was great! I would use the \\\"Table of Contents (2)\\\" extension to enable the users to more easily navigate the Jupyter Notebook.\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/toc2/README.html\\n\\nhttps://jupyter-contrib-nbextensions.readthedocs.io/en/latest/\", \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p9\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I would show statistics about term frequency to users to help writing labeling functions. Also, I would like to make some helper functions to get synonyms and antonyms easily available, so that a user can improve coverage of simple keyword-matching strategy.\", \"feedback\": \"It was hard to write complex functions because of the time limit. I wanted to see overall statistics of the term frequency, but I was not able to check the statistics easily in time.\\nSo, I just used simple keyword matching as labeling functions\", \"other\": \"Tutorial and explanations were thorough. The researcher remained unbiased and objective.\", \"metric\": \"easy to learn\", \"value\": 2}, {\"participant\": \"p9\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"I'd add the 'not have' condition. It was hard to find out 'must-have' keywords for the 'not spam' label. I'd like to add some statistical characteristics (word count, text length) of a data record and synthesize in/equalities using them.\", \"feedback\": \"I tried to construct concepts with relevant keywords for each label. This strategy worked for certain label (spam) but didn't work well for another label (not spam). I also tried to build rules based on entity labels, but it didn't work well.\", \"other\": null, \"metric\": \"easy to learn\", \"value\": 5}, {\"participant\": \"p4\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"The pre-defined functions provided by the responsible for the experiment helped a lot. Having more pre-defined functions would be very useful (even for programmers).\", \"feedback\": \"I'm used using Python, but I usually need to double check many of the commands syntax, so it makes the process of generating the rules a little bit slower.\", \"other\": \"I liked it.\", \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p4\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Allowing the addition of:\\n1)  \\\"negative examples rules\\\";\\n2) general rules (not associated to any specific example);\\n3) a \\\"python window\\\" in which you could use python code (as used in Snorkel), thus explore best of both worlds (the easiness and speed of current Ruler, and the expressiveness of Snorkel)\", \"feedback\": \"I enjoyed using the tool! I could quickly define a set of rules with reasonable Precision/Recall over the available data. It would take much longer to get to the same performance without the help of Ruler. \\nOne type of rule that I could not create is for negative examples. I tried to create a rule that would be a negative example of spam. In the controlled experiment scenario (as it is a binary classification task) I could get the same effect by set the \\\"negative example rule\\\" for one class as a \\\"positive example\\\" rule to the other class.\\nAlso, I tried to create a rule (based on my domain knowledge) that was not specifically associated to a instance, but I could not.\", \"other\": null, \"metric\": \"easy to learn\", \"value\": 5}, {\"participant\": \"p6\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1. Give formal definitions to key terms. \\n2. Prepare a step-by-step tutorial.  \", \"feedback\": \"Work\\n1. The tool can capture keyword-based functions. \\n2. The tool supports AND and OR operators. \\n\\nDidn't work\\n1. Some terms are not well-defined (e.g. Concept).\\n2. The tool lacks step-by-step documents.  \", \"other\": \"The tool is cool. \", \"metric\": \"easy to learn\", \"value\": 3}, {\"participant\": \"p6\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"1. Reduce unnecessary coding as much as possible. \\n2. Make the evaluation of labeling functions instant. \", \"feedback\": \"Worked\\n1. Snorkel supports python that I am familiar with. \\n2. Snorkel enables me to leverage programming skills to label data.\\n\\nDidn't work\\n1. Snorkel is coding intensive that I have to run multiple Snorkel cells to evaluate labelling functions. \\n2. Snorkel does not instantly evaluate labeling functions. I have to rerun evaluation codes each time I update labeling functions. \", \"other\": \"The Snorkel tool is cool. \", \"metric\": \"easy to learn\", \"value\": 2}, {\"participant\": \"p5\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to learn\", \"value\": 5}, {\"participant\": \"p5\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to learn\", \"value\": 5}, {\"participant\": \"p0\", \"condition\": \"Ruler\", \"comments\": \"It was great exercise for me! thank you!\", \"how to improve\": \"When I mouse over token(s), I wished I had a popup to categorize it\", \"feedback\": \"Categorizing tokens was hard.for me\", \"other\": null, \"metric\": \"easy to learn\", \"value\": 2}, {\"participant\": \"p0\", \"condition\": \"Snorkel\", \"comments\": \"Great experiments! I will look into snorkel as I have some ML tasks. Thanks!\", \"how to improve\": \"If non-essential codes (e.g. evaluation codes) were defined out of the notebooks, they would be more easy to understand.\", \"feedback\": \"I firstly write functions so that recalls get high.\", \"other\": null, \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p1\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": null, \"feedback\": null, \"other\": null, \"metric\": \"easy to learn\", \"value\": 3}, {\"participant\": \"p1\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Scalability: the system becomes slower towards the end. Further optimization & approximation could be considered.\", \"feedback\": \"I noticed that my label accuracy did not constantly improving: first improves and then drops. Maybe this is just an extreme case, but I feel it is important to validate if other users also show similar trend.\", \"other\": null, \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p2\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"I was confused with the metric shown in the second block of the Apply function section. The comment \\\"Don't worry\\\" was not enough for me to disregard the value. :p\\n\\nMy work was going back and forth between labeling function, applying function and training a classifier. I would be helpful if the cells that the user runs are compiled into a single function (on a single cell) so I could simply call the function.\\n\\nFor example, prepare a function that traverses the namespace to list up any functions that begin with lf (or a longer prefix if it conflicts with something).\\n\", \"feedback\": \"My strategy was to directly take a look at examples and came up with salient words/phrases to write down keyword-based labeling functions. \", \"other\": \"The instructions are clear and the user study is organized well. I'm curious about the (psychological) effect of the time limit and being monitored/recorded.\\n\\nI may not pay attention but the input argument `x` of each labeling function was not clear at the beginning, which took a couple of minutes to figure out.\", \"metric\": \"easy to learn\", \"value\": 2}, {\"participant\": \"p2\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"1)\\nIt was not very intuitive how the system makes use of the labeling functions I made. What I was confused is when I saw the recall of class 0 dropped after adding labeling function for class 0. I had to conduct label-function engineering to figure out the best combination.\\n\\n2)\\nAs I asked during the user study, it would be helpful if I could directly add rules that are not activated by the current example (which I could with Snorkel.)\\n\\n3)\\nSomehow, it seems that the data has more positive examples than negative examples. It will be helpful if the system has a search function to retrieve a negative example (that contains certain words etc.)\\n\\n4)\\nI found several examples from which I didn't want to create labeling functions, but I would like to simply label the examples. This may be for the user study but only allowing the user to create labeling functions may not be the best way.\\n\\n5)\\nSimilarly, in practice, it should be better having a base classifier/base dictionary as a starting point. For example (I worked on a sentiment analysis task today), using a pre-trained classifier (trained on other sentiment classifier dataset) and/and/or using sentiment dictionary that contains words with sentiment polarity information. I feel like what I did with the system is approximately close to reconstructing an affective dictionary from scratch (tuned toward the dataset, in one sense.)\\n\", \"feedback\": \"My strategy is simply adding salient words/phrases of each class while monitoring the dev set performance. As I added labeling functions, I tried to create more detailed rules as I progress.\", \"other\": \"The user study was well organized and instructions were clear.\\n\\nI'm wondering if the user study randomly shuffles the order of methods. Although two tools/datasets are different, I feel like I was more prepared to work on the task.\", \"metric\": \"easy to learn\", \"value\": 5}, {\"participant\": \"p8\", \"condition\": \"Snorkel\", \"comments\": null, \"how to improve\": \"Providing more stats/exploration options in terms of helping the user improve coverage. Instead of simply showing overlapping and conflicts, it would be better to see the samples and stats of two LFs by their overlap or conflict. Also it could automatically search for ensembling the labeling functions and provide suggestions to the user.\", \"feedback\": \"I tried to create a bunch of labeling functions on filtering by certain tokens at beginning and see how each works; then I modified those that turn out to have most wrong labels and got better results. Also I tried to have labeling functions on length and non-letters of the text, which seems to be not very useful. I should have tried to ensemble the labeling functions, which would have improve the coverage a lot.\", \"other\": null, \"metric\": \"easy to learn\", \"value\": 4}, {\"participant\": \"p8\", \"condition\": \"Ruler\", \"comments\": null, \"how to improve\": \"Like snorkel, I think auto suggestions on improving the coverage of LFs could be very helpful. And it may be useful to allow users combine LFs or edit LFs in python in order to reduce the execution time and to be more flexible for users with coding experience. The regex concept should be very useful but I only use it for several times, and I think it would be great if the tool can suggest regex expressions based on user's annotation.\", \"feedback\": \"I think it is task dependent. For sentiment analysis, the coverage of labeling functions on certain tokens or phrases can be relatively small and may not be accurate as there are many variants in the phrase and the negation would affect the result. \", \"other\": \"I feel that there could be 10-15 more time after the tutorial for users to play with the tool on the example task. In both user studies I figured out ways to improve the performance shortly after it is finished.\", \"metric\": \"easy to learn\", \"value\": 5}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvhR-ypNdxHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}